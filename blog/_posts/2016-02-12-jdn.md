---
layout: post
title: Comment le Big Data entre dans l'ère du temps réel
tags: [data, press]
comments: false
image: /img/realtime.jpg
---

Banques, distributeurs, e-commerçants… De plus en plus d'acteurs intègrent la composante temps réel dans les projets de Big Data. Si des briques open source comme Spark, Storm ou Flink sont arrivées à maturité, il reste des défis à relever.

Le Big Data gagne en vitesse pour devenir le "fast data". Alors que nombre d'entreprises ne font qu'envisager l'apport du traitement de données en volume, d'autres associent en plus à cette vision une composante temps réel. Les salles de marché des institutions financières font déjà depuis plus d'une trentaine d'années de l'enrichissement de donnée en temps réel en passant par des développements spécifiques.

Si des éditeurs comme Tibco promettent depuis quelques années de faire du décisionnel temps réel, l'arrivée à maturité de frameworks open source comme Spark, Storm ou Flink démocratise les usages et ouvre le champ des possibles.

## Big Data temps réel : des projets tous azimuts

Big Data & data insights manager chez Ysance, Ryadh Dahimene voit les projets associant Big Data et temps réel se multiplier. "Dans le secteur de la grande distribution, nous avons travaillé sur la prédiction en temps réel des risques de rupture de stock en analysant les tickets de caisse. L'effet sur le chiffre d'affaires est immédiatement quantifiable", constate l'expert.

Les banques exploitent, elles, leur masse de données pour lutter contre la fraude et le blanchiment d'argent. Dans l'e-commerce, le Big Data temps réel permet par exemple de personnaliser un site web en capturant toutes les interactions d'un internaute afin de dégager rapidement son profil. "Ce que font très bien Amazon ou Criteo qui jouent sur cette temporalité pour faire des recommandations de produits dans l'instant", poursuit Ryadh Dahimene.

Un assureur sera, lui, intéressé par les informations météo et les données en matière d'accidentologie. Ysance a, ainsi, monté une application de démonstration en utilisant les données des courses des taxis de New-York, livrées en open data, et des informations de recensement d'accidents. Objectif : identifier les lieux les plus accidentogènes et leur impact sur le business des "Yellow Cabs". En France, Velib' (JCDecaux) libère ses données en temps réel (les disponibilités de ses stations par exemple) via une API proposée à des applications tierces.

## Mixer mode batch et streaming

Pour Mathias Herberts, CTO et cofondateur de la start-up Cityzen Data (une société spécialisée dans le Big Data et l'IoT), il faut s'entendre sur la notion de temps réel. "Pour le trading haute fréquence, c'est de l'ordre de la nano seconde. Pour un département marketing habitué à avoir un reporting à J+7, le temps réel cela peut être à J+1", analyse le consultant.

Intégrer la composante temps réel nécessite toutefois de relever un certain nombre de défis. Il faut tout d'abord s'assurer de la disponibilité des données. Si les données émises par les objets connectés intègrent nativement la composante temps (via la notion de séries temporelles ou time series), leur stockage et leur indexation immédiats nécessitent un cadre particulier. "Pour faire face au flux permanent émis par les objets connectés, il faut mettre en œuvre une architecture à même de collecter et consolider les données et dimensionner les clusters qui absorberont cette volumétrie", estime Ryadh Dahimene.

Une fois les données fédérées et disponibles, il faut s'accorder sur leur mode de traitement. "Le Big Data temps réel peut sembler simple à premier vue", avance Mathias Herberts. "Il s'agit de gérer des flux de données. Mais un lot de données fraîches peut aussi venir enrichir des informations historisées. Cela complexifie l'algorithmie. Il faut gérer à la fois l'instantanéité et le passé. Et ce, en limitant l'empreinte mémoire, et en ne conservant pas forcément l'exhaustivité des données", souligne cet ancien de Google.

## Storm : une solution mâture basée sur la notion de topologie
Le temps réel n'exclut donc pas le batch. Il est possible de mixer les deux modes, traitement par lot et streaming. Ce que d'aucuns appellent "architecture lambda". Flink ou Apache Beam, sur lequel Google investit beaucoup pour son offre DataFlow, ont cette approche mixte. Par défaut, ces frameworks gèrent des flux, considérant qu'un batch, ce n'est qu'un flux qui s'arrête. "A la différence de Spark qui a, dans un premier temps, géré uniquement des batchs, puis a fait entrer le traitement des flux au chausse pied", note Mathias Herberts. Sortie en juillet dernier, la version 2.0 de Spark a toutefois rectifié le tir.

Storm trouve davantage grâce aux  yeux de Mathias Herberts. Cette autre solution open source soutenue par la fondation Apache repose sur le principe de topologie. Elle se présente sous la forme d'un graphe. Les nœuds racines de ce graphe émettent un stream (un flux de logs par exemple) qui va déclencher l'exécution d'un ou plusieurs composants appartenant à une même topologie. "Cela permet d'aller à plus bas niveau et de configurer la plateforme plus finement", estime Mathias Herberts.

## Kafka et Spark, le couple incontournable

Si Ryadh Dahimene reconnaît la maturité de Storm, il voit dans les projets qu'il est amené à étudier la place incontournable qu'occupe le couple associant Kafka et Spark. "Kafka collecte les données et les agrège dans des flux sous une forme consommable, puis Spark pourra prendre en charge le traitement. Avec deux sous-ensembles que sont Spark Streaming et Spark MLlib (API machine learning), il devient possible de pousser la porte du prédictif en temps réel", insiste le manager Big Data d'Ysance.

A ses yeux, Flink fait figure de challenger. "Ce framework apparaît comme nouveau alors qu'il existait avant le boom de Spark sous le nom de Stratosphere. Il était développé par des chercheurs de l'université de TU Berlin", rappelle-t-il. "Flink n'a pas la maturité de Spark qui fédère une grosse communauté. Il reste néanmoins un framework prometteur qui porte une vision européenne de la question." La société allemande Data Artisans qui industrialise des services pour Flink a, par exemple, Zalando comme client.

Pour les entreprises n'ayant pas les compétences ou les ressources ad hoc, se pose la question de recourir aux fournisseurs de cloud pour déployer une architecture Big Data temps réel. Amazon Web Services, Microsoft Azure ou Google Cloud Plaform proposent tous des services managés de Big Data intégrant la dimension temps réel en s'appuyant sur les frameworks open source évoqués plus haut. Des offres qui permettent de tester un "use case" sans avoir à investir dans une infrastructure dédiée.

Xavier Biseul (La Rédaction)
JDN

{: .box-note}
**Note:**  Repris ici, cet article est originellement paru sur: [https://www.journaldunet.com/solutions/dsi/1189126-comment-le-big-data-entre-dans-l-ere-du-temps-reel/](https://www.journaldunet.com/solutions/dsi/1189126-comment-le-big-data-entre-dans-l-ere-du-temps-reel/)